---
layout: post
title:  Regularization Effect of Adversarial Examples Learning
author: <a href="http://chandlerzuo.github.io/">Chandler</a>
---

**Adversarial Examples**

Neural networks can be susceptible to adversarial examples, which are perturbed input data too small to be detected by human but tricky enough to confuse a neural network model to make wrong prediction. The following is a famous example from [Goodfellow et al. 2014]][2]. While the left photo is categorized correctly in the panda category, with some specifically constructed noise, the right photo is incorrectly categorized as in the gibbon category. In fact, such noises can be constructed to confuse the network in an arbitrary way. [Daniel Geng and Rishi Veerapaneni's blog](https://ml.berkeley.edu/blog/2018/01/10/adversarial-examples/) is a nice tutorial for how to generate adversarial examples for image classification problems.

![](https://ml.berkeley.edu/blog/assets/2017-10-31-adversarial-examples/goodfellow.png) 

The key step of constructing adversarial examples is to find the **direction** of the input noise that the network is sensitive to. A neural network input often comes in high dimensions; for an RGB image with 128x128px, the dimension is 128x128x3=49152. With such high dimensions, it is expected that a neural network cannot be robust against noises in all dimensions. To find the most malicious direction of the input noise is essentially to find the dimensions in the input data which a neural network is most sensitive to. Mathematically, such malicious directions can be found by calculating the derivative of the objective function, which a neural network tries to minimize, against the input data. The derivative calculation is the right tool to find malicious directions because by definition, it is used to find how minimally perturbed input data can maximally change the function output.

**Fast Gradient Sign Method and LASSO**

We know that neural network models are sensitive to adversarial examples. How can we make neural network to be robust against adversarial noises? Since we already know how to generate such adversarial noises, we can simply augment the training data using the synthetic adversarial examples. This idea was proposed by [Goodfellow et al][2]. Specifically, they proposed the **Fast Gradient Sign Method (FGSM)**. This method generate noise that has the same scale in every dimension, but the sign in each dimension is guided by the gradient calculation. The mathematical formula for an adversarial example is:

![][eqn1]

In a 1-layer neural network model with sigmoid activation, [Goodfellow et al][2] reckons that such learning is closely related to LASSO penalty. This is because the objective function with FGSM examples is:

![][eqn2]

Comparing this to the objective function of [LASSO-penalized likehood estimation](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3873110/):

![][eqn3]

FGSM simply changes the order between Sigmoid activation and imposing the LASSO penalty. Because of this, [Goodfellow et al][2] states that FGSM has regularization effect similar to LASSO. The difference between these two types of regularization is that the penalty in FGSM automatically deactivates if the prediction is confident enough, while LASSO penalty focuses more on the worst case scenarios.

**Generalization of FGSM**

The relationship between FGSM and LASSO motivates me to study the asymptotic behavior of FGSM in simple regression models. In fact, despite much numerical results showing the benefit of using FGSM for real data, there is not much theoretical justification. For LASSO, decades of theoretical research have yielded  much understanding on its benefits, which have well explained its role in neural network regularization (commonly known as L1 weight decay). If we can derive statistical properties for FGSM in simple regression models, such as linear regression and logistic regression, the insights may help us understand why it has been successful in deep neural network, and what some of its limitations are.

[My recent paper][1] takes this approach. The theoretical framework is built for Generalized FGSM estimation for [generalized linear models (GLM)](https://en.wikipedia.org/wiki/Generalized_linear_model). The theory is built for estimation with the following objective function:

![][eqn4]

This setting is more general than FGSM with sigmoid activation in that

1. the activation function can not only be the sigmoid function;
2. the penalty function can not only be LASSO.

The first point is actually weak; the framework includes linear regression besides logistic regression, but does not include non differentiable activation functions such as ReLU. The second point is, however, more interesting. Before, we have seen that if the penalty function is LASSO, the objective function corresponds to learning using FGSM adversarial examples. If we choose other penalty function, such as general Lr shrinkage or [SCAD] penalty, the corresponding schema to generate adversarial noises is:

![][eqn5]

This gives a large number of schemas to generate adversarial examples, which can be interesting to evaluate empirically.

**Asymptotics of FGSM-type Learning**

It is already know that penalized likelihood estimation for GLM has three properties:

- *Root-n consistency*. In layman's term, the standard error for the parameter estimator is halved every time the training sample is quadrupled. This is a desirable statistical property for regression models, and is the best rate achievable because of [Cramer-Rao's Bound](https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound).
- *Sparsity*. The parameter estimator is sparse. This creates some bias but may reduce the overall estimation error due to [bias-variance tradeoff](http://www.cs.cmu.edu/~wcohen/10-601/bias-variance.pdf).
- *Oracle property* for non-concave penalties. If the true parameter is sparse, the estimated parameter is almost as accurate as if we know which parameters are zero. This only happens if the penalty function is non-concave, such as SCAD or Lr with 0<r<1.

The main result of [My paper][1] is that all these properties are inherited by Generalized FGSM. Moreover, the asymptotic distribution is highly similar to penalized likelihood estimation. For both methods, the asymptotic distribution follows the form

![][eqn6]

For Generalized FGSM, the asymptotic maximized function with Lr penalty is:

![][eqn7]

For penalized likelihood estimation, the corresponding asymptotic maximized function is:

![][eqn8]

**Comparing Generalized FGSM with Penalized Likelihood Estimation**

The asymptotic results have two implications.

1. The multipler of the penalty in Generalized FGSM is automatically scaled by the dispersion of error. This is desirable since the penalty function is uncoupled from noise level in the data. The tuning parameter lambda can be selected based on the sparsity level alone. For penalized likelihood estimation in practice, on the other hand, the selected tuning parameter lambda need be scaled against a heuristically estimated noise level.
2. Generalized FGSM is different from penalized likelihood estimation if the training data is not *sign neutral*. This concept is related to the quantity V in the asymptotic equations:
![][eqn9]
When V is non-zero, it introduces additional bias compared to penalized likelihood estimation. For linear regression models, V=0 requires trivial conditions that can be assumed true in most practical cases. However, for logistic regression, V=0 requires that the data sampling need be balanced in a way.

Sign neutrality requires that the average sign of error across all training data is roughly 0. To see how balanced the sampling should be, consider a single binary random variable Y with probability p for Y=1 and 1-p for Y=0. The expected sign of the error Y-p is 1*p+(-1)*(1-p)=2p-1. This is equal to 0 only when p=0.5. Intuitively, if the training data sampling is sign neutral, the average binary probability across all observations should be roughly 0.5. This condition is not satisfied for any data that are not balanced across classes.

One thing to mention here is that, such additional bias does not necessarily make Generalized FGSM inferior to penalized likelihood. Just as LASSO penalty, biases in estimation can trade off with variance to improve overall estimation accuracy. Whether the additional bias in Generalized FGSM improves such tradeoff requires future analysis.

**Conclusions**

Asymptotic analysis is an important topic in theoretical statistics. Yet, I have often heard critism for such research in that such theory yields little practical usage. As an applied researcher myself, however, I do feel such critism is unfair. Methodological research sometimes move much faster than mathematical theory. But it is not wise to undermine the theory, because it help answer three questions: *why certain methodology works, how it can be generalized, and where its boundary is*.

There is no doubt that FGSM and the general adversarial examples learning is highly successful. With so many successful applications of this method, I have kept asking myself the three questions above. This motivates me to develop the theory in [this paper][1]. It is far from comprehensive of course, but it does shed light on to these three questions. Root-n consistency, sparsity, and oracle properties are all nice properties that explains its success; its generalization can be induced by a large number of penalty functions already used in statistics; sign neutrality might be the boundary for its application. The theory so far is entirely based on 1-layer neural network models, but I believe such properties should give insights for deep neural network models as well.

[1]: https://www.dropbox.com/s/1dtcb8s6ccpwbke/lasso_fgsm.pdf?dl=0
[2]: https://arxiv.org/abs/1412.6572
[eqn1]: https://dl.dropboxusercontent.com/s/jsmuuooq3803wia/eqn1.png {: height="4px"}
[eqn2]: https://dl.dropboxusercontent.com/s/1ymuf6bf86mbvjp/eqn2.png {: height="2px"}
[eqn3]: https://dl.dropboxusercontent.com/s/y0u6khbx10x27f3/eqn3.png {: height="10px"}
[eqn4]: https://dl.dropboxusercontent.com/s/1i3zxapk140vxdb/eqn4.png {: height="5px"}
[eqn5]: https://dl.dropboxusercontent.com/s/68xv54cwgglsoi0/eqn5.png {: height="10px"}
[eqn6]: https://dl.dropboxusercontent.com/s/ywfhaioalvylv4a/eqn6.png {: height="10px"}
[eqn7]: https://dl.dropboxusercontent.com/s/2d5kbbvt200ea8s/eqn7.png {: height="8px"}
[eqn8]: https://dl.dropboxusercontent.com/s/f02bo32t8e1jmtw/eqn8.png {: height="30px"}
[eqn9]: https://dl.dropboxusercontent.com/s/o6wbuej87sepby1/eqn9.png {: height="10px"}

*(c)2017-2026 CHANDLER ZUO ALL RIGHTS PRESERVED*